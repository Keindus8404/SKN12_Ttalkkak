# -*- coding: utf-8 -*-
"""Ttalkkak_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10kyAsXihpGg6ScPfhEKUDsv1e7UkCnp9
"""

from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer
import torch

# config/tokenizer 로드
config = AutoConfig.from_pretrained(".", num_labels=2)
tokenizer = AutoTokenizer.from_pretrained(".")

# 모델 아키텍처 생성 후 .pt 로드
model = AutoModelForSequenceClassification.from_config(config)
state = torch.load("Ttalkkak_model_v1.pt", map_location="cpu")
model.load_state_dict(state)
model.eval()

# 예측
input_text = "이 내용은 누가 담당이죠?"

inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)
with torch.no_grad():
    logits = model(**inputs).logits
print("예측 라벨:", torch.argmax(logits, dim=-1).item())